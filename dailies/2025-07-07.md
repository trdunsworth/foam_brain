# 2025-07-07

Started the day by going with Ellen to her Untrasound. Everything smiled nicely per the tech.

I have several things on the list for today. Working on the reports at first, then I have to get busy with the things I need to address the ideas I have for NAVIGATOR 26 for the IAED. I have two separate ideas. The first is the Mental Health procedures analyses. The second is examining the impact of reopened calls.

## Packt Publishing Interview Notes

1. I think that centers have to find their greatest pain points. Right now, many companies providing solutions are focusing on intercepting non-emergency calls since, in the US and Canada, the same telecommmunicator pool is repsonsible for both emergency and non-emergency lines. By intercepting these calls, a center with fewer telecommunicators on duty can improve service to the emergency lines. During my dissertation, I identified a few different forecasting libraries including TimeGPT that is LLM-based that can be used at a free tier to create a serviceable forecast. Blue Sky analytics platform can also be used by centers at no cost.
2. I have built two AI labs, one personal and one in the office, to leverage open-source models locally so I can have greater control of the costs associated with using models. I try to be careful and creative about my use of LLMs so I don't incur uncontrolled costs in my labs and I also use that as an example to ensure anything we develop and deploy in-house is going to be cost-efficient.
3. Right now, I've been using more lightweight models since most of our development targets are relatively small and don't require so many resources both size and token-wise. I am interested in quantizing models as I continue to develop more advanced models because I know that I may have to deploy solutions in resource-challenged environments. Because we can, in this sector of public service, access PII, PHI, and possibly even CJIS data, we either have to partner with vendors who can satisfy strict governmental requirements or deploy locally to protect our information.
4. The biggest challenge that I've encountered is resource-based. If I have a model that cannot perform with the resources I have available, it becomes frustrating. That's one reason I'm trying my hand at quantizing models in my personal laboratory. I think that upkeep is going to be the biggest challenge. So far, my only failures and obstacles have been easy to address. I can change models, if possible, and re-evaluate the performance of the app. Right now, support is part of the development process. I do believe that it is something that should be considered before deploying anything into production. It's not enough to build it, you have to take care of it too. You also have to plan for upgrades and requests for enhancements along the way.
5. Because I am also the analyst that works with the data that I need to synthesize, I ensure that I am familiar with the intricacies of that data. I have several reports that I run normally and I take those outputs and combine them with scripts that I have written to create a framework that can pass for real data. I compare it agqinst real data as well by comparing my output to a random real dataset. That enables me to fine-tune my scripts to ensure the output is reasonably close to authentic data. I started building my own scripts when I discovered that an online generator returned normally distributed intervals between events. I also changed how I built the timestamps in the dataset by focusing on the elapsed times between events and then making the timestamps from there. Much of my authentic data is log-normally or exponentially distributed, so I have to check each run to ensure that I stay on track.
6. The biggest challenge that I face is pushback from peers when I present synthetic data. The irony is that I use it in presentations to prevent losing my audience if they recognize an event, even when the data is cleaned. There was also some push back when I would show histograms or density plots of some data and it was normally distributed. The audience skepticism led me to writing my own software and to ensuring that I paid better attention to what authentic data looked like. Now, when I explain my reasons, I am met with an appreciation and questions about how I synthesize the data and if others can use the generator for training purposes. I also had a group interested in using it to augment tabular data to ensure their tools were performing properly. Since I used an open-source license, I only ask that improvements get sent back to me. I think that the community is starting to understand that synthetic data can allow us to do a lot that were otherwise challneging. I think that revisiting the models that allow you to generate the data is just as important as revisiting any other models that are deployed. I also have to be aware of restrictions on what models I can use and how data is shared. My own personal lab is much more daring than my workplace lab because I can select different models for each and see how the performance is differnt.
7. Initially, I received some pushback, but I think that had to do with the first datasets that I created. They weren't as realistic and robust as I would have liked. However, as I improved them and continued to advocate for using them, my audiences accepted the logic and now they are completely fine with synthetic data as a presentation and teaching tool. I have also started using them as testing tools to ensure that other software I develop is working as designed. There are vendors in our space who are using a blend of synthetic and authentic data to create training materials for new telecommunicators and I'm very excited to see that. I have heard sample training material that sounds very interesting. When augmented with other authentic and synthetic data, I'm certain these are going to be great training platforms in the future. One way that I ensure that my data is realistic, but not too realistic is to employ specific libraries to generate some of the data. That enables me to tweak the settings in my own software to prevent the data from being too realistic.
8. While I cannot speak for those in the private sector, I know that I try to follow the same ethical guidelines that I was taught throughout my university days. I test everything in sandboxes before deployment of any software I develop. Then I watch the performance after deployment, and I take feedback from my users about anything that works and doesn't work as intended or advertised. I have started reviewing NIST's frameworks to ensure that my own development models stay within industry recommendations. I also have a good relationship with our security team so my in-house work gets additional welcome vetting. In speaking with my private sector counterparts, we all seem to be in agreement that we have to be very careful in our decisions to ensure that we do not compromise anything in our development cycles. Thankfully, the discussions in which I participate in this sphere are always about being very careful. For example, while there are several vendors, as I discussed earlier, who are looking to alleviate the congestion that comes from non-emergency calls to our centers, nobody has ever suggested putting anything in front of 9-1-1 emergency calls. We want to help our telecommunicators, not replace them or get between them and the people who need them.
9. While I don't have direct experience in this area mainly because most of what I have deployed is still in testing, I do know that a couple of deployments that had to be rolled back when performance didn't meet either the centre or the vendor's expectations. I think that the lessons that came from that were that the models were good for genreal purposes, but there are things special to this industry that were not accounted for in the first deployments. Subsequent devleopment has started to improve the space and will lead to more improvements over time. I think that we all have to learn how to translate many of the intricacies of this industry better to improve the performance of the models we develop and employ.
10. I am most excited about three things. One the improvement in handling non-emergency calls so they can be deferred or handled without a telecommmunicator's intervention. The second is better real-time information getting into the hands of telecommunicators and first responders. The last is improvements in AI-assisted translation. As that improves, it creates a better environment for both telecommunicators and callers. Personally, I think that there may be a lot of creative ways to improve the delivery of these options into the hands of the telecommunicators who need it the most.

## Mental Health Call Analyses

I think this can be pretty straight-forward in the design. Find out from Mike about the non-dispatch operational components. Pull data about call volumes and processing times prior to and after launches. Review outcomes from the call narratives. We do have to ensure that the data is scrubbed very well.

## Report Restructuring

I want this to be a project with multiple moving parts. The first question is going to be arranging the project structure store the data properly and make it easily accessible. I think we need to put the libraries off into their own container. `{r} renv` is going to be important in the development cycle. I want a pdf or docx file that encompasses a weekly drill down of the dataset for the previous week. I know that it can be altered over time to add or remove different foci as management sees fit. I think that it can also address my correlative question below. I want to extend it to allow for comparison to the week prior and a year prior for the same time frame. I might have to find ways to ingest data from ECaTS into Quarto to see if I can add that data. I can ask the same questions from Rapid's Eclipse product if it works better. I also want to create a Shiny dashboard and a self-serve website that would allow users to select a past dataset and ask questions about it.  I don't know if I want to make that AI-enabled yet, but it is something to think about down the road. 

## Call Re-opening vs. Call Cloning Impact Study

This should be relatively straight-forward as well. Grab the data for a time period. Go back and find where the data needs to be corrected and then compare the two datasets for their analytical power and show how the negative and missing data create problems in the process.

### Additional Ideas today

Is there a correlation between call volume and response time? We tend to generalize, with broad sweeping statements that the more calls a centre gets, the more time it takes to parse and process them. However, anecdotally, when I'm looking at the numbers, I don't really see much of a correlation. Can I undertake a study that addresses that and determines if there is a correlation either positive or negative and what the strength of that is?

#### Note to self

I need to get back to using this more efficiently and more frequently.
